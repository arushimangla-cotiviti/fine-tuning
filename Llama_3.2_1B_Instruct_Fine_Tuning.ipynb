{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0009bb-7790-411a-874a-3b8d4e0f4dbd",
   "metadata": {},
   "source": [
    "# Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "22f14c72-b660-4a8f-a877-679796d60789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.34.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.15.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.45.4)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.4.1.post300)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759520ff-62eb-403e-80a2-426b32a67fd6",
   "metadata": {},
   "source": [
    "# Logging into Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "44bb73a8-9d47-425e-ade4-5c03e9110791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `fine-tuning` has been saved to /home/sagemaker-user/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/sagemaker-user/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `fine-tuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"xxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b7053-951c-4b1c-a988-0451a073f93d",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "74cae994-5ddb-4611-8bc9-3c557521a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 22:21:34.177275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-27 22:21:34.190147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-27 22:21:34.208749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-27 22:21:34.214494: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-27 22:21:34.227445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n",
    "from peft import LoraModel, get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f65682-197c-4d1b-840b-5ee90ef2fa56",
   "metadata": {},
   "source": [
    "# Initializing Sagemaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "cece0aa7-fda0-4eea-b9c2-b745d5c21134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::637423395717:role/service-role/AmazonSageMaker-ExecutionRole-20250324T120618\n",
      "sagemaker bucket: sagemaker-us-east-1-637423395717\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20250124T132142')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e9abf-b078-41c2-a635-d76dbad50e71",
   "metadata": {},
   "source": [
    "# Getting Data from S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "9f22e425-6e06-48e7-82d1-08dd053d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sagemaker-bucket-fine-tuning'\n",
    "train_file_key = 'data_for_gdf_mapping.csv'  \n",
    "gdf_file_key = 'gdf_master_file.csv'\n",
    "\n",
    "response = s3.get_object(Bucket=bucket_name, Key=train_file_key)\n",
    "response_1 = s3.get_object(Bucket=bucket_name, Key=gdf_file_key)\n",
    "\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "csv_content_1 = response_1['Body'].read().decode('ISO-8859-1')\n",
    "train_df = pd.read_csv(StringIO(csv_content))\n",
    "gdf_master_data = pd.read_csv(StringIO(csv_content_1))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "c4fb4bb0-624b-4311-8369-3b4e81ac6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del trainer\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88afb1a-76f2-48b9-bf15-3ff84b8d9612",
   "metadata": {},
   "source": [
    "# Performing Inference on the Model to get Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "execution_state": "idle",
   "id": "b3630be3-3a44-4e74-a145-230515255614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf562c2d5fb4f2586e15348997317ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0affd57c17e1433db5c78ca21c7933bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883c63acf3ed44f6ac826425f320fa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dec22938171456db827badcfc0309ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a20a0279947a98ddb973ff79f1991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3daa53dab84f1480f98feab285c74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Inference Result:\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "You are an expert in mapping medical data fields. The task is to map a raw field to a standardized GDF field used by our organization. You can get more context about the fields from the raw field description and gdf field description. If the raw field descrption is NaN or is not given or is empty, then try to understand what the raw_field might contain from its name and then try mapping it to the appropriate GDF field. The list of gdf fields used within the organization, along with thier description is in                           file                         gdf_field  \\\n",
      "0                        Claim  CC_CLAIM_FINAL_VERSION_INDICATOR   \n",
      "1                        Claim               CC_CUSTOM_AMOUNT_01   \n",
      "2                        Claim               CC_CUSTOM_AMOUNT_02   \n",
      "3                        Claim               CC_CUSTOM_AMOUNT_03   \n",
      "4                        Claim               CC_CUSTOM_AMOUNT_04   \n",
      "...                        ...                               ...   \n",
      "1220  PostpayExclusionCriteria       EX_PROVIDER_IDENTIFIER_TYPE   \n",
      "1221  PostpayExclusionCriteria               EX_SOURCE_FILE_NAME   \n",
      "1222  PostpayExclusionCriteria      EX_SOURCE_FILE_PRODUCED_DATE   \n",
      "1223  PostpayExclusionCriteria               EX_TERMINATION_DATE   \n",
      "1224                       NaN                               NaN   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                               gdf_desc  \\\n",
      "0     This field indicates whether this claim (and line) is the most current version being sent in this data feed.  This field should be populated with a 1 by the client if this is the current latest version of the claim (and line).  All other versions of the claim (and line) that are not current should be populated with a 0.   \n",
      "1                                                                                                                                                                Client defined amount field.  Use of this field should be discussed with Cotiviti  to ensure proper utilization and storage prior to the addition of data in the file.   \n",
      "2                                                                                                                         The value in this field represents the Client defined amount field.  Use of this field should be discussed with Cotiviti  to ensure proper utilization and storage prior to the addition of data in the file.   \n",
      "3                                                                                                                         The value in this field represents the Client defined amount field.  Use of this field should be discussed with Cotiviti  to ensure proper utilization and storage prior to the addition of data in the file.   \n",
      "4                                                                                                                         The value in this field represents the Client defined amount field.  Use of this field should be discussed with Cotiviti  to ensure proper utilization and storage prior to the addition of data in the file.   \n",
      "...                                                                                                                                                                                                                                                                                                                                 ...   \n",
      "1220                                                                                                                                                                                                                                                  The value in this field represents Provider Identifier type TIN, Provider ID, etc   \n",
      "1221                                                                                                                                                                                                                                                             This field represents the exact name of the client NoTouch file loaded   \n",
      "1222                                                                                                                                                                                                                                                                             This represents the date the NoTouch file was received   \n",
      "1223                                                                                                                                                                                                                                                       The value in this field represents the date when the exclusion should end on   \n",
      "1224                                                                                                                                                                                                                                                                                                                                NaN   \n",
      "\n",
      "                dtype size                       grp meta_data importance  \n",
      "0                Text    1                    Client         N          C  \n",
      "1     Numeric 9(8)V99   10                    Client         N        NaN  \n",
      "2     Numeric 9(8)V99   10                    Client         N        NaN  \n",
      "3     Numeric 9(8)V99   10                    Client         N        NaN  \n",
      "4     Numeric 9(8)V99   10                    Client         N        NaN  \n",
      "...               ...  ...                       ...       ...        ...  \n",
      "1220             Text   25  PostpayExclusionCriteria         Y          M  \n",
      "1221             Text   80  PostpayExclusionCriteria         N          M  \n",
      "1222             Date    8  PostpayExclusionCriteria         N          M  \n",
      "1223             Date    8  PostpayExclusionCriteria         N          M  \n",
      "1224              NaN  NaN                       NaN       NaN        NaN  \n",
      "\n",
      "[1225 rows x 8 columns]. Take your time, understand the raw_field to be mapped and then based on the infromation you have map the raw field to the appropriate gdf field correctly.\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " Raw field: clvc_amount_05. This field represents a Client defined field. Please map it to the appropriate GDF field. Based on the description of the raw field, I think it might be related to the provider identifier, but I am not sure. Can you help me understand the raw field and then map it to the appropriate GDF field?\n",
      "\n",
      "## Step 1: Understand the raw field\n",
      "The raw field clvc_amount_05 represents a Client defined field. This suggests that the field is related to the provider identifier.\n",
      "\n",
      "## Step 2: Identify the provider identifier\n",
      "The raw field clvc_amount_05 is likely related to the provider identifier, as it is described as a Client defined field. The description of the raw field indicates that it represents the Client defined amount field, which is used to identify the provider.\n",
      "\n",
      "## Step 3: Map the raw field to the appropriate GDF field\n",
      "Based on the description of the raw field, I believe that the appropriate GDF field is CC_PROVIDER_IDENTIFIER_TYPE. This field represents the provider identifier.\n",
      "\n",
      "The final answer is: CC_PROVIDER_IDENTIFIER_TYPE\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "system_prompt = f\"<|start_header_id|>system<|end_header_id|>\\nYou are an expert in mapping medical data fields. The task is to map a raw field to a standardized GDF field used by our organization. You can get more context about the fields from the raw field description and gdf field description. If the raw field descrption is NaN or is not given or is empty, then try to understand what the raw_field might contain from its name and then try mapping it to the appropriate GDF field. The list of gdf fields used within the organization, along with thier description is in {gdf_master_data}. Take your time, understand the raw_field to be mapped and then based on the infromation you have map the raw field to the appropriate gdf field correctly.\"\n",
    "user_prompt = \"<|start_header_id|>user<|end_header_id|>\\n Raw field: clvc_amount_05. This field represents a Client defined field. Please map it to the appropriate GDF field.\"\n",
    "\n",
    "input_text = system_prompt + \"\\n\" + user_prompt\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "baseline_result = generator(input_text, truncation=True, padding=True, max_new_tokens=1000, num_return_sequences=1)\n",
    "# inputs = tokenizer(input_text, truncation=True, padding=True, max_length=1024, return_tensors=\"pt\")\n",
    "# output = model.generate(**inputs, max_length=1000, num_beams=5, early_stopping=True)\n",
    "\n",
    "# decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(f\"Inference Result: {decoded_output}\")\n",
    "\n",
    "print(\"Baseline Inference Result:\")\n",
    "print(baseline_result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a141568-1fca-4ab2-8d2c-2801d530bebd",
   "metadata": {},
   "source": [
    "# Training the Model on RCA-14 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "82c385da-50ce-46ef-b142-6c8675ec68bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/27/25 22:21:57] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Parameter <span style=\"color: #008700; text-decoration-color: #008700\">'function'</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #e100e1; text-decoration-color: #e100e1; font-weight: bold\">function</span><span style=\"color: #000000; text-decoration-color: #000000\"> tokenize_function at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fef6bec5080</span><span style=\"font-weight: bold\">&gt;</span> <a href=\"file:///opt/conda/lib/python3.11/site-packages/datasets/fingerprint.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">fingerprint.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/datasets/fingerprint.py#328\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">328</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         of the transform datasets.arrow_dataset.Dataset._map_single         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         couldn't be hashed properly, a random hash was used instead. Make   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         sure your transforms and parameters are serializable with pickle or <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         dill for the dataset fingerprinting and caching to work. If you     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         reuse this transform, the caching mechanism will consider it to be  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         different from the previous calls and recompute everything. This    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         warning is only showed once. Subsequent hashing failures won't be   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         showed.                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/27/25 22:21:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Parameter \u001b[38;2;0;135;0m'function'\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;38;2;225;0;225mfunction\u001b[0m\u001b[39m tokenize_function at \u001b[0m\u001b[1;36m0x7fef6bec5080\u001b[0m\u001b[1m>\u001b[0m \u001b]8;id=910777;file:///opt/conda/lib/python3.11/site-packages/datasets/fingerprint.py\u001b\\\u001b[2mfingerprint.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=145349;file:///opt/conda/lib/python3.11/site-packages/datasets/fingerprint.py#328\u001b\\\u001b[2m328\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         of the transform datasets.arrow_dataset.Dataset._map_single         \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         couldn't be hashed properly, a random hash was used instead. Make   \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         sure your transforms and parameters are serializable with pickle or \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         dill for the dataset fingerprinting and caching to work. If you     \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         reuse this transform, the caching mechanism will consider it to be  \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         different from the previous calls and recompute everything. This    \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         warning is only showed once. Subsequent hashing failures won't be   \u001b[2m                  \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         showed.                                                             \u001b[2m                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633f03cc09004719b1409610ba7e0eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "df = pd.DataFrame(train_df)\n",
    "df['raw_desc'] = df['raw_desc'].fillna('')\n",
    "df['text'] = df['raw_field'] + \" \" + df['raw_desc']\n",
    "\n",
    "# Prepare the inputs (X) and outputs (y)\n",
    "X = df['text'].tolist()\n",
    "y = df['gdf_field'].tolist() \n",
    "\n",
    "# Map labels to integers (for classification)\n",
    "label_map = {label: i for i, label in enumerate(set(y))}\n",
    "df['label'] = df['gdf_field'].map(label_map)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(X, df['label'], test_size=0.2)\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels.tolist()})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2b68a-7413-40b6-8eac-94136fe53cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
